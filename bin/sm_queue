#! /usr/bin/env python

# System imports
import os
import os.path
import sys
import socket
import json
from datetime import datetime, timezone
import time as time
import argparse
import psutil
import copy
import shutil
from configobj import ConfigObj
from validate import Validator

# Third-party imports
import daemon
import lockfile

# Local imports
from shakemap.utils.config import (get_config_paths,
                                   get_configspec,
                                   config_error)
import shakemap.utils.queue as queue
from shakelib.rupture.origin import write_event_file
from shakemap.utils.amps import AmplitudeHandler
from shakelib.rupture import constants

MEMORY_UPDATE_TIME = 0
ASSOCIATE_UPDATE_TIME = 0
DB_MAINTENANCE_TIME = 0


def do_periodic_tasks(handler, logger, data_path, config, children,
                      shake_config):
    """ Check for finished children and start any needed timed repeats.

    Args:
        handler (AmplitudeHandler object): An object of type AmplitudeHandler.
        logger (object): The process logging object.
        data_path (str): The path where the ShakeMap data directories
                         live.
        config (dict): The configuration data.
        children (dict): The data structure maintaining the state of child
                         processes.
        shake_config (dict): The parsed shake.conf configuration file.

    Returns:
        nothing: Nothing.
    """
    #
    # Do routine stuff: check for finished children,
    # check for reruns
    #
    global MEMORY_UPDATE_TIME
    global ASSOCIATE_UPDATE_TIME
    global DB_MAINTENANCE_TIME

    queue.reap_children(children, config, logger)

    current_time = int(time.time())
    repeats = handler.getRepeats()
    for eventid, otime, rep_list in repeats:
        if rep_list[0] < current_time:
            event = handler.getEvent(eventid)
            if eventid in children:
                #
                # Event is already running; pop this repeat and move on
                #
                rep_list.pop(0)
                if len(rep_list) == 0:
                    rep_list = None
                event['repeats'] = rep_list
                handler.insertEvent(event, update=True)
                continue
            logger.info('Running repeat of event %s' % eventid)
            # Update the XML because the DB may have newer information
            write_event_xml(data_path, event, logger)
            queue.dispatch_event(eventid, logger, children,
                                 'Scheduled repeat', config, shake_config)
            rep_list.pop(0)
            if len(rep_list) == 0:
                rep_list = None
            event['repeats'] = rep_list
            event['lastrun'] = current_time
            handler.insertEvent(event, update=True)

    #
    # Print out memory usage to see how much we're leaking...
    #
    if MEMORY_UPDATE_TIME + 3600 < current_time:
        MEMORY_UPDATE_TIME = current_time
        process = psutil.Process(os.getpid())
        mem = getattr(process.memory_full_info(), 'uss', 0) / 1048576.0
        logger.info('Currently using %.1f MB' % mem)

    #
    # Run the associator and get a list of events with new data
    #
    if config['associate_interval'] >= 0 and \
       ASSOCIATE_UPDATE_TIME + config['associate_interval'] < current_time:
        ASSOCIATE_UPDATE_TIME = current_time
        associate_all(handler, data_path, logger, config, children,
                      shake_config)

    #
    # Do the occasional DB cleanup once per day; keep amps for 30
    # days and events for 1 year
    #
    if DB_MAINTENANCE_TIME + 86400 < current_time:
        DB_MAINTENANCE_TIME = current_time
        #
        # First do the assocication to make sure we don't drop any
        # amps that might associate
        #
        if config['associate_interval'] >= 0:
            ASSOCIATE_UPDATE_TIME = current_time
            associate_all(handler, data_path, logger, config, children,
                          shake_config)
        #
        # Now clean out the amps and events
        #
        handler.cleanAmps(threshold=30)
        handler.cleanEvents(threshold=365)

    return


def associate_all(handler, data_path, logger, config, children, shake_config):
    """Do the associateAll method of the the AmplitudeHandler and
    then process all of the events with updated data.

    Args:
        handler (AmplitudeHandler object): An object of type AmplitudeHandler.
        data_path (str): The path where the ShakeMap data directories
                         live.
        logger (object): The process logging object.
        config (dict): The configuration data.
        children (dict): The data structure maintaining the state of child
                         processes.
        shake_config (dict): The parsed shake.conf configuration file.

    Returns:
        nothing: Nothing.
    """
    event_list = handler.associateAll(pretty_print=True)
    for eventid in event_list:
        event = handler.getEvent(eventid)
        process_origin(event, 'Data association', handler, data_path,
                       logger, config, children, shake_config)

    return


def write_event_xml(data_path, event, logger):
    """ Create the event directory if it doesn't exist and write/re-write
    the event.xml file

    Args:
        data_path (str): The path where the ShakeMap data directories
                         live.
        event (dict): The event data structure.
        logger (object): The process logging object.

    Returns:
        nothing: Nothing.
    """
    ttemp = event['time']
    try:
        dt = datetime.strptime(event['time'], constants.TIMEFMT)
    except ValueError:
        try:
            dt = datetime.strptime(event['time'], constants.ALT_TIMEFMT)
        except ValueError:
            logger.error("Can't parse input time %s" % event['time'])
            return
    event['time'] = dt

    event_dir = os.path.join(data_path, event['id'], 'current')
    if not os.path.isdir(event_dir):
        os.makedirs(event_dir)
    event_xml = os.path.join(event_dir, 'event.xml')

    logger.info('Writing event %s to event.xml' % (event['id']))
    val = write_event_file(event, event_xml)
    if val:
        logger.error('Error writing event.xml: %s' % val)

    event['time'] = ttemp

    return


def move_event_directory(oldid, newid, data_path, logger):
    try:
        shutil.move(os.path.join(data_path, oldid),
                    os.path.join(data_path, newid))
    except shutil.Error as e:
        logger("Error trying to move data directory %s to %s: %s" %
               (oldid, newid, str(e)))
    return


def process_origin(event, action, handler, data_path, logger, config,
                   children, shake_config):
    """ Determine if an event should be processed (or reprocessed) and
    dispatch it for processing.

    Args:
        event (dict): The event data structure.
        action (str): The "type" of the trigger that caused this function
                      to be called.
        handler (AmplitudeHandler object): An instance of the
                                           AmplitudeHandler object.
        data_path (str): The path where the ShakeMap data directories
                         live.
        logger (object): The process logging object.
        config (dict): The (cleaned-up) configuration object.
        children (dict): The data structure maintaining the state of child
                         processes.
        shake_config (dict): The parsed shake.conf config file.

    Returns:
        nothing: Nothing.
    """
    current_time = int(time.time())
    force_run = False
    #
    # See if we already have this event, make a decision
    #
    existing = handler.getEvent(event['id'])
    if existing is None and 'alt_eventids' in event:
        for eid in event['alt_eventids'].split(','):
            if eid == event['id']:
                continue
            alt_exists = handler.getEvent(eid)
            if alt_exists is None:
                continue
            # If the old event is currently running, kill it
            if eid in children:
                children[eid]['popen'].kill()
                children[eid]['popen'].wait()
            # Delete the old event from the database
            handler.deleteEvent(eid)
            # Move the old event directory to the new ID
            move_event_directory(eid, event['id'], data_path, logger)
            # Now treat the the new event ID as a new event.
            existing = None
            # But force it to run (because we want to update the event ID),
            # bypassing date and magnitude checks
            force_run = True
            break

    if existing is None:
        #
        # This is a new event
        #
        update = False
        # Do we want to run this event?
        if not force_run and queue.magnitude_too_small(event['mag'],
                                                       event['lon'],
                                                       event['lat'],
                                                       config):
            logger.info('Event %s (mag=%f) too small, skipping' %
                        (event['id'], event['mag']))
            return
        if not force_run and queue.event_too_old_or_in_future(event, config):
            logger.info('Event %s too old or too far in the future, skipping' %
                        event['id'])
            return
        #
        # Looks like we'll be running this event, get the repeats
        # (if any) and toss the ones that have already passed
        #
        replist = None
        try:
            dt = datetime.strptime(event['time'], constants.TIMEFMT)
        except ValueError:
            try:
                dt = datetime.strptime(event['time'], constants.ALT_TIMEFMT)
            except ValueError:
                logger.error("Can't parse input time %s" % event['time'])
                return
        event_timestamp = int(dt.replace(tzinfo=timezone.utc).timestamp())
        for mag in sorted(config['repeats'].keys(), reverse=True):
            if event['mag'] > mag:
                replist = [x + event_timestamp for x in config['repeats'][mag]
                           if event_timestamp + x > current_time]
                if len(replist) == 0:
                    replist = None
                break
        event['repeats'] = replist
    else:
        #
        # We've run this event before
        #
        update = True
        mtw = config['max_trigger_wait']
        #
        # Whatever we decide below, we want to update the event
        # info in the database
        #
        event['lastrun'] = existing['lastrun']
        event['repeats'] = copy.copy(existing['repeats'])
        if event['id'] in children:
            #
            # Event is currently running, don't run it but make sure
            # there's a repeat pretty soon
            #
            if event['repeats']:
                if event['repeats'][0] > current_time + mtw:
                    event['repeats'].insert(0, current_time + mtw)
            else:
                event['repeats'] = [current_time + mtw]
            handler.insertEvent(event, update=update)
            logger.info('Event %s is currently running, shelving this '
                        'update' % event['id'])
            return
        if event['repeats']:
            delta_t = current_time - event['repeats'][0]
            if delta_t > -mtw:
                # We're due for a rerun anyway, so just insert the
                # new data in the database and return
                handler.insertEvent(event, update=update)
                logger.info('Event %s will repeat soon, shelving this '
                            'update' % event['id'])
                return
        if current_time - event['lastrun'] < mtw:
            #
            # We ran this event very recently, but don't have a repeat
            # scheduled in the near future, so let's skip this one
            # but make sure something happens relatively soon
            #
            if event['repeats']:
                event['repeats'].insert(0, current_time + mtw)
            else:
                event['repeats'] = [current_time + mtw]
            handler.insertEvent(event, update=update)
            logger.info('Event %s ran recently, shelving this update' %
                        event['id'])
            return
    #
    # Run this event
    #
    event['lastrun'] = current_time

    handler.insertEvent(event, update=update)

    write_event_xml(data_path, event, logger)

    queue.dispatch_event(event['id'], logger, children, action, config,
                         shake_config)

    return


def process_other(data, action, handler, data_path, logger, config, children,
                  shake_config):
    """A trigger has been issued for an event. Treat this as
    an origin update. If the event in question is not in our
    database, ignore the message.

    Args:
        eventid (str): The event ID of the triggered event.
        action (str): The "type" of the trigger that caused this function
                      to be called.
        handler (AmplitudeHandler object): An instance of the
                                           AmplitudeHandler object.
        data_path (str): The path where the ShakeMap data directories
                         live.
        logger (object): The process logging object.
        config (dict): The (cleaned-up) configuration object.
        children (dict): The data structure maintaining the state of child
                         processes.
        shake_config (dict): The parsed shake.conf config file.

    Returns:
        nothing: Nothing.
    """
    eventid = data['id']
    existing = handler.getEvent(eventid)
    if existing:
        process_origin(existing, action, handler, data_path, logger, config,
                       children, shake_config)
    else:
        if 'alt_eventids' in data:
            for eid in data['alt_eventids'].split(','):
                if eid == eventid:
                    continue
                existing = handler.getEvent(eid)
                if existing:
                    process_origin(existing, action, handler, data_path,
                                   logger, config, children, shake_config)
                    return
        logger.info('Trigger is for unprocessed event %s: ignoring' %
                    data['id'])
    return


def process_cancel(data, handler, logger, config, children, shake_config):
    """We've received a cancellation of an event: run 'shake cancel'.

    Args:
        data (dict): The dictionary must have an event ID
                     under the 'id' key.
        handler (AmplitudeHandler object): An instance of the
                                           AmplitudeHandler object.
        logger (object): The process logging object.
        config (dict): The (cleaned-up) configuration object.
        children (dict): The data structure maintaining the state of child
                         processes.
        shake_config (dict): The parsed shake.conf config file.

    Returns:
        nothing: Nothing.
    """
    eventid = data['id']
    existing = handler.getEvent(eventid)
    if existing:
        queue.dispatch_event(eventid, logger, children, 'cancel', config,
                             shake_config)
        return

    if 'alt_eventids' in data:
        for eid in data['alt_eventids'].split(','):
            if eid == eventid:
                continue
            existing = handler.getEvent(eid)
            if existing:
                queue.dispatch_event(eid, logger, children, 'cancel', config,
                                     shake_config)
                return

    logger.info('cancel is for unprocessed event %s: ignoring' % eventid)
    return


class Dummycontext(object):
    """This is a dummy context that can be used as a context manager
    with 'with'. It doesn't do anything.
    """
    def __enter__(self): return self

    def __exit__(*x): pass


def get_context(context, attached):
    """Returns a context based on the value of the 'attached' argument.

    Args:
        context (Context manager): A valid context manager.
        attached (bool): If attached is True, then the function returns
                         an instance of the Dummycontext; if it is False
                         the function returns the 'context' argument.

    Returns:
        Context manager: If attached is True, the function returns an
                         instance of the Dummycontext; if False, returns
                         the 'context' argument.
    """
    if attached:
        return Dummycontext()
    else:
        return context


def get_parser():
    """Make an argument parser.

    Returns:
        ArgumentParser: an argparse argument parser.
    """
    description = """
    Run a daemon process to accept origin, rupture, fault, etc., messages
    and run shake on the event.
    """
    parser = argparse.ArgumentParser(
        description=description,
        formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument('-a', '--attached', action='store_true',
                        help='Inhibit daemonization and remain attached '
                             'to the terminal.')
    parser.add_argument('-b', '--break_lock', action='store_true',
                        help="If the pid lock file can't be acquired, break "
                             "the existing lock and acquire a new lock. Do "
                             "this only if you are certain tha no other "
                             "sm_queue processes are running.")
    return parser


def main(pargs):
    children = {}

    install_path, data_path = get_config_paths()

    config = queue.get_config(install_path)
    #
    # Get shake.conf for the autorun modules
    #
    config_file = os.path.join(install_path, 'config', 'shake.conf')
    spec_file = get_configspec('shake')
    shake_config = ConfigObj(config_file, configspec=spec_file)
    results = shake_config.validate(Validator())
    if not isinstance(results, bool) or not results:
        config_error(shake_config, results)
    #
    # Turn this process into a daemon
    #
    logpath = os.path.join(install_path, 'logs')
    if not os.path.isdir(logpath):
        os.makedirs(logpath)
    pidfile = os.path.join(logpath, 'queue.pid')
    filelock = lockfile.FileLock(pidfile)
    if filelock.is_locked():
        if pargs.break_lock:
            filelock.break_lock()
        else:
            logger = queue.get_logger(logpath, pargs.attached)
            logger.error("pid lock file '%s' exists, can't start sm_queue; "
                         "exiting..." % (pidfile))
            sys.exit(-1)

    context = daemon.DaemonContext(working_directory=data_path,
                                   pidfile=filelock)

    with get_context(context, pargs.attached):
        logger = queue.get_logger(logpath, pargs.attached)
        #
        # Create the socket
        #
        qsocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        qsocket.bind(('', config['port']))
        # Set a timeout so that we can occasionally look for other
        # things to do
        qsocket.settimeout(30)
        qsocket.listen(5)
        #
        # Get a connection to the event database
        #
        handler = AmplitudeHandler(install_path, data_path)

        logger.info('sm_queue initiated')

        while True:
            #
            # Do routine stuff
            #
            do_periodic_tasks(handler, logger, data_path, config, children,
                              shake_config)
            #
            # Now wait for a connection
            #
            try:
                (clientsocket, address) = qsocket.accept()
            except socket.timeout:
                #
                # Normal timeout; do routine tasks and then go
                # back to waiting for a connection
                #
                continue
            #
            # Got a connection
            #
            hostname, _, _ = socket.gethostbyaddr(address[0])
#            hostname = socket.getfqdn(hostname)
            logger.info('Got connection from %s at port %s' %
                        (hostname, address[1]))

            if hostname not in config['servers']:
                logger.warning('Connection from %s refused: not in valid '
                               'servers list' % hostname)
                clientsocket.close()
                continue

            #
            # The accept() should guarantee that there's something
            # to read, but something could go wrong...
            #
            try:
                clientsocket.settimeout(5)
                data = clientsocket.recv(queue.MAX_SIZE)
            except socket.timeout:
                logger.warning('Did not get data from connection, continuing')
                clientsocket.close()
                continue
            else:
                clientsocket.close()
            #
            # Decode the data and do something
            #
            try:
                cmd = json.loads(data.decode('utf8'))
            except json.decoder.JSONDecodeError:
                logger.warning("Couldn't decode data from %s: ignoring" %
                               hostname)
                continue

            if not isinstance(cmd, dict) or 'type' not in cmd or \
               'data' not in cmd or 'id' not in cmd['data']:
                logger.warning('Bad data from %s: ignoring' % hostname)
                continue

            if cmd['type'] == 'origin':
                logger.info('Received "origin" for event %s' %
                            cmd['data']['id'])
                if 'action' in cmd['data']:
                    action = cmd['data']['action']
                else:
                    action = 'origin'
                process_origin(cmd['data'], action, handler, data_path,
                               logger, config, children, shake_config)
            elif cmd['type'] == 'cancel':
                logger.info('Received "cancel" for event %s' %
                            cmd['data']['id'])
                process_cancel(cmd['data'], handler,  logger,
                               config, children, shake_config)
            else:
                logger.info('Received "%s" for event %s' %
                            cmd['type'], cmd['data']['id'])
                process_other(cmd['data'], cmd['type'], handler, data_path,
                              logger, config, children, shake_config)


if __name__ == '__main__':

    parser = get_parser()
    pargs = parser.parse_args()

    main(pargs)
